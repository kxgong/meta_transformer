<!DOCTYPE html>
<html>
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-WJDETMSJZZ"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-WJDETMSJZZ');
  </script>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Meta-Transformer: A Unified Framework for Multimodal Learning</title>
  <link rel="icon" type="image/x-icon" href="static/images/cuhk.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Meta-Transformer: A Unified Framework for Multimodal Learning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=KuYlJCIAAAAJ" target="_blank">Yiyuan Zhang</a><sup>1,2*</sup>,</span>
                <span class="author-block">
                  <a href="https://kxgong.github.io/" target="_blank">Kaixiong Gong</a><sup>1,2*</sup>,</span>
                  <span class="author-block">
                    <a href="http://kpzhang93.github.io/" target="_blank">Kaipeng Zhang</a><sup>2</sup>,
                  </span>
                </br>
                  <span class="author-block">
                    <a href="http://www.ee.cuhk.edu.hk/~hsli/" target="_blank">Hongsheng Li</a><sup>1,2</sup>,
                  </span>
                    <a href="https://mmlab.siat.ac.cn/yuqiao/index.html" target="_blank">Yu Qiao</a><sup>2</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://wlouyang.github.io/" target="_blank">Wanli Ouyang</a><sup>2</sup>,
                  </span>
                  <span class="author-block">
                    <a href="http://people.eecs.berkeley.edu/~xyyue/" target="_blank">Xiangyu Yue</a><sup>1</sup>
                  </span>
            </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Multimedia Lab, The Chinese University of Hong Kong   
                      <br> <sup>2</sup>OpenGVLab, Shanghai AI Laboratory</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2307.10802" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="https://arxiv.org/abs/2307.10802" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/invictus717/MetaTransformer" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2307.10802" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>

                  <a href="https://twitter.com/_akhaliq/status/1682248055637041152"><img src="https://img.icons8.com/color/48/000000/twitter.png" width="45" height="45"></a>
                  <a href="https://www.youtube.com/watch?v=V8L8xbsTyls&ab_channel=CSBoard"><img src="https://img.icons8.com/color/48/000000/youtube-play.png" width="45" height="45"></a>
                  <a href="https://huggingface.co/kxgong/Meta-Transformer"> <img src="static/images/huggingface.png" width="45" height="45"> </a>
                  <a href="https://open.spotify.com/episode/6JJxcy2zMtTwr4jXPQEXjh"> <img src="https://upload.wikimedia.org/wikipedia/commons/1/19/Spotify_logo_without_text.svg" width="40" height="40">
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!--
<video poster="" id="tree" autoplay controls muted loop height="100%">

  <source src="static/videos/banner_video.mp4"
  type="video/mp4">
</video>
-->

<!-- Teaser video-->
<section class="hero teaser square_teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/teaser.png" alt="Teaser"/>
      <h2 class="subtitle has-text-centered">
        Unified Multimodal Learning. Meta-Transformer utilizes the same backbone to encode natural languages, images, point clouds, audio, video, infrared, hyperspectral, X-ray, time-series, tabular, and graph data. It reveals the potential of transformer architectures for universal perception. 
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<section class="section">
<h2 class="title is-4 has-text-centered">
Modalities
</h2>
<div class="multi_column">
  <div class="has-text-centered">
    <a href="https://en.wikipedia.org/wiki/ImageNet" target="_blank">
      <p> Image</p>
    </a>
  </div>
  <div class="has-text-centered">
    <a href="https://en.wikipedia.org/wiki/Point_cloud", target="_blank">
      <p> Point Cloud</p>
    </a>  
  </div>
  <div class="has-text-centered">
    <a href="https://en.wikipedia.org/wiki/Medical_image_computing", target="_blank">
      <p> Medical Image</p>
    </a>
  </div>
  <div class="has-text-centered">
    <a href="https://en.wikipedia.org/wiki/Hyperspectral_imaging", target="_blank">
      <p> Hyperspectral</p>
    </a>
  </div>
  <div class="has-text-centered">
    <a href="https://en.wikipedia.org/wiki/Graph_(abstract_data_type)", target="_blank">
    <p> Graph Data</p>
    </a>
  </div>
  <div class="has-text-centered">
    <a href="https://en.wikipedia.org/wiki/Audio_mining#Audio_Classification", target="_blank">
    <p> Audio</p>
    </a>
  </div>
</div>

<div class=" multi_column">
    <div>
      <div class="hvr-grow modality">
        <a href="https://en.wikipedia.org/wiki/ImageNet" target="_blank">
        <img src="static/images/image.svg" alt="point cloud" id="modality_image"/>
        </a>
      </div>
    </div>
    <div >
      <div class="hvr-grow modality">
        <a href="https://en.wikipedia.org/wiki/Point_cloud", target="_blank">
          <img src="static/images/point_cloud.svg" alt="point cloud" id="modality_image"/>
        </a>
      </div>
    </div>
    <div >
      <div class="hvr-grow modality">
        <a href="https://en.wikipedia.org/wiki/Medical_image_computing", target="_blank">
          <img src="static/images/medical.svg" alt="point cloud" id="modality_image"/>
        </a>
      </div>
    </div>
    <div >
      <div class="hvr-grow modality">
        <a href="https://en.wikipedia.org/wiki/Hyperspectral_imaging", target="_blank">
          <img src="static/images/hyper.svg" alt="point cloud" id="modality_image"/>
        </a>
      </div>
    </div >
    <div >
      <div class="hvr-grow modality">
        <a href="https://en.wikipedia.org/wiki/Graph_(abstract_data_type)", target="_blank">
          <img src="static/images/graph.svg" alt="point cloud" id="modality_image"/>
        </a>
      </div>
    </div>
    <div >
      <div class="modality">
        <a href="https://en.wikipedia.org/wiki/Audio_mining#Audio_Classification", target="_blank">
          <img src="static/images/audio.svg" alt="point cloud" id="modality_image"/>
        </a>
      </div>
    </div>
    

</div>

<div class="multi_column">
  <div class="has-text-centered">
    <a href="https://en.wikipedia.org/wiki/Video_content_analysis", target="_blank">
    <p> Video</p>
    </a>
  </div>

  <div class="has-text-centered">
    <a href="https://en.wikipedia.org/wiki/Thermographic_camera", target="_blank">
    <p> Infrared Image</p>
    </a>
  </div>

  <div class="has-text-centered">
    <a href="https://en.wikipedia.org/wiki/Natural_language_processing", target="_blank">
    <p> Language</p>
    </a>
  </div>
  
  <div class="has-text-centered">
    <a href="https://en.wikipedia.org/wiki/Time_series", target="_blank">
    <p> Time-series</p>
    </a>
  </div>

  <div class="has-text-centered">
    <a href="https://ego4d-data.org/docs/data/imu/", target="_blank">
    <p> IMU</p>
    </a>
  </div>

  <div class="has-text-centered">
    <a href="https://en.wikipedia.org/wiki/Table_(information)", target="_blank">
    <p> Tabular Data</p>
    </a>
  </div>
</div>

<div class="multi_column">
  <div >
    <div class="hvr-grow modality">
      <a href="https://en.wikipedia.org/wiki/Video_content_analysis", target="_blank">
        <img src="static/images/video.svg" alt="point cloud" id="modality_image"/>
      </a>
    </div>
  </div>
  <div >
    <div class="hvr-grow modality">
      <a href="https://en.wikipedia.org/wiki/Thermographic_camera", target="_blank">
        <img src="static/images/infrared.svg" alt="point cloud" id="modality_image"/>
      </a>
    </div>
  </div>
  <div >
    <div class="hvr-grow modality">
      <a href="https://en.wikipedia.org/wiki/Natural_language_processing", target="_blank">
        <img src="static/images/text.svg" alt="point cloud" id="modality_image"/>
      </a>
    </div>
  </div>
  <div >
    <div class="hvr-grow modality">
      <a href="https://en.wikipedia.org/wiki/Time_series", target="_blank">
        <img src="static/images/time-series.svg" alt="point cloud" id="modality_image"/>
      </a>
    </div>
  </div>
  
  <div >
    <div class="hvr-grow modality">
      <a href="https://ego4d-data.org/docs/data/imu/", target="_blank">
        <img src="static/images/dynamic.svg" alt="point cloud" id="modality_image"/>
      </a>
    </div>
  </div>

  <div >
    <div class="modality">
      <a href="https://en.wikipedia.org/wiki/Table_(information)", target="_blank">
        <img src="static/images/tabular.svg" alt="point cloud" id="modality_image"/>
      </a>
    </div>
  </div>
</div>
</section>



<!-- Paper abstract -->
<section class="section hero ">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Multimodal learning involves utilizing data from various modalities to improve model capacity. Despite the years of development in this field, it remains challenging to devise a unified framework for processing natural language, 2D images, 3D point clouds, and audio spectrograms due to crucial gaps among these different modalities. This study proposes a novel approach that demonstrates a network with frozen parameters can encode the data from the aforementioned four modalities and achieve favorable performance, resulting in a unified framework called Meta Transformer. Using this framework, the raw input data from various modalities are converted to a shared token space, allowing a subsequent encoder with frozen parameters to extract high-level semantic features of the input data. Composed of three main components: a unified data tokenizer, a modality-shared encoder, and task-specific heads for downstream tasks, Meta Transformer is the first framework for unified learning among the four modalities with unpaired data, to the best of our knowledge. We evaluated Meta Transformer on different benchmarks across modalities, such as ImageNet for classification, GLUE for text understanding, ModelNet-40, S3DIS, ShapeNetPart for point cloud, and Speech Commands V2 for speech spectrograms. These results indicate a promising future for developing unified multimodal intelligence with transformers.
          </p>
          <img width="750px" src="static/images/Meta-Transformer_application.png" alt="MY ALT TEXT"/>
          <p>
            Meta-Transformer can be applied to many application fields, including 3D recognition, nighttime securaity, weather prediction, etc.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<div class="columns is-centered has-text-centered">
  <HR align=center style="border:3 double #000000" width="80%" SIZE=5>
</div>


<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column ">
        <h2 class="title is-3">Video Presentation</h2>
        <div class="center-div">
                <iframe width="750px" height="415" src="https://www.youtube.com/embed/V8L8xbsTyls" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
              </div>      
      </div>
    </div>
  </div>
</section>

<div class="columns is-centered has-text-centered">
  <HR align=center style="border:3 double #000000" width="80%" SIZE=5>
</div>


<!-- Video end
<div class="center-div">
</div>
<br>
<table align="center" width="600px">
  <tbody>
      <tr>
          <td>
              <div class="center-div">
                <iframe width="750px" height="415" src="https://www.youtube.com/embed/V8L8xbsTyls" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
              </div>               
          </td>
      </tr>
      <tr>
  </tbody>
</table> -->

<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Meta-Transformer</h2>
        <div class="content has-text-justified">
          <img src="static/images/framework.png" alt="MY ALT TEXT"/>
          <p>
            Illustration of Unified Multimodal Learning framework for natural language, images, point clouds, and audio spectrograms. An all-to-one tokenizer is used to convert the raw input data from different modalities to a shared token space. Then, a modality-shared encoder with frozen parameters is used to extract high-level semantic features of the input data. Finally, task-specific heads are used for downstream tasks. This framework enables perceiving different modalities with one shared encoder and without paired data.
          </p>
        </div>
        <h2 class="title is-3">Tokenizer</h2>
        <div class="content has-text-justified">
          <img src="static/images/tokenizer.png" alt="MY ALT TEXT"/>
          <p>
            We propose the meta scheme in (a) containing grouping, convolution, and transformation progress. Then (b)-(e) represents the building blocks applied with our meta scheme on texts, images, point clouds, and audio spectrograms.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<div class="columns is-centered has-text-centered">
  <HR align=center style="border:3 double #000000" width="80%" SIZE=5>
</div>


<!-- Result -->
<!-- <section class="section hero is-light"> -->
<section class="hero section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-12">
        <h2 class="title is-3">Experiment</h2>
        <div class="content has-text-justified">
          <p>
            We evaluate Meta-Transformer on a wide range of modalities, including 2D images, natural language, 3D point clouds, audio spectrograms, time-series data, etc.
          </p>
          <img src="static/images/exp_cmp.png" alt="MY ALT TEXT"/>
          <p>
            Compared with current state-of-the-art methods, Meta-Transformer also delivers an outstanding performance.
          </p>
          <img src="static/images/tab_2.png" alt="MY ALT TEXT"/>
          <p>
            Table 1: Experimental results for text understanding on the GLUE benchmark. We compare existing advanced methods from paraphrasing, sentiment, duplication, inference, and answering tasks, and we report the pre-training settings and performances.
          </p>
          <img src="static/images/tab_3.png" alt="MY ALT TEXT"/>
          <p>
            Table 2: Experimental results for image understanding. We conduct experiments in classification, object detection, and instance segmentation tasks on the ImageNet [23], MSCOCO [71], and ADE20K [74] datasets. ∗ denotes zero-shot image classification, † denotes linear probing for image classification, and ‡ indicates the model is pre-trained on ImageNet-22K [23], where Bold and underline indicate best and second best results.
          </p>
        </div>
        <div class="content has-text-justified is-light">
          <img src="static/images/tab_4.png" alt="MY ALT TEXT"/>
          <p>
            Table 3: Experimental results for infrared and hyperspectral image understanding.We conduct experiments in classification tasks on the SYSU-MM01 and Indian Pine datasets. We report Rank-1 (R@1), Top-1 Accuracy scores, and the number of trainable parameters (Params).
          </p>
          <img src="static/images/tab_5.png" alt="MY ALT TEXT"/>
          <p>
          Table 4: Experimental results for point cloud understanding. We conduct experiments on the ModelNet-40 [25], S3DIS [26], and ShapeNetPart [27] datasets. We compare existing advanced methods from classification, semantic, and object part segmentation tasks, and we report the pretraining modality (Pre-train) and trainable parameters number (Param.) of each method.
          </p>
        </div>

        <img src="static/images/tab_time_series.png" alt="MY ALT TEXT"/>
          <p>
          Table 6: Time-series Forecasting with Meta-Transformers. Following TimesNet, we report the number of trainable parameters and average performances from 4 different prediction lengths, which is {96, 192, 336, 720}.
        </p>

        
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- 
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">

        <img src="static/images/carousel1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item">
 
        <img src="static/images/carousel2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item">

        <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item">

      <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div>
  </div>
</div>
</div>
</section>
 -->




<!-- 
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">

      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">

            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
 -->


<!-- Video carousel -->
<!--
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
           
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
           
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
 -->



<!-- Paper poster -->
<!--
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
-->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      If you find our work useful, please cite our paper. BibTex code is provided below:
      <pre><code>@article{zhang2023metatransformer,
        title={Meta-Transformer: A Unified Framework for Multimodal Learning}, 
        author={Zhang, Yiyuan and Gong, Kaixiong and Zhang, Kaipeng and Li, Hongsheng and Qiao, Yu and Ouyang, Wanli and Yue, Xiangyu},
        year={2023},
        journal={arXiv preprint arXiv:2307.10802},
  }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
